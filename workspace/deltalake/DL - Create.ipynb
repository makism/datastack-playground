{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c998c14f-7129-47bb-9c04-8a1ec669433d",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bc5a55-5b35-4e38-a13b-b094a11ac7d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:41:38.886301Z",
     "iopub.status.busy": "2023-04-12T07:41:38.885909Z",
     "iopub.status.idle": "2023-04-12T07:41:38.945653Z",
     "shell.execute_reply": "2023-04-12T07:41:38.944883Z",
     "shell.execute_reply.started": "2023-04-12T07:41:38.886264Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b316699-7d96-4c97-9262-5bcf7fcdc811",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7dff68-aad3-4529-9575-e887774dd29d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:41:38.947697Z",
     "iopub.status.busy": "2023-04-12T07:41:38.947352Z",
     "iopub.status.idle": "2023-04-12T07:41:43.335503Z",
     "shell.execute_reply": "2023-04-12T07:41:43.334906Z",
     "shell.execute_reply.started": "2023-04-12T07:41:38.947675Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f4627c01-039e-4e69-a167-16368d67769e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 288ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f4627c01-039e-4e69-a167-16368d67769e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 07:41:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"pyspark-notebook\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a789b784-bab8-4c46-8763-7783151b444c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:41:43.336528Z",
     "iopub.status.busy": "2023-04-12T07:41:43.336333Z",
     "iopub.status.idle": "2023-04-12T07:41:43.343365Z",
     "shell.execute_reply": "2023-04-12T07:41:43.342572Z",
     "shell.execute_reply.started": "2023-04-12T07:41:43.336508Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"minio\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"minio123\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://172.18.0.2:9000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b123370-590c-4664-bc9f-977f5ff634d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:41:43.344642Z",
     "iopub.status.busy": "2023-04-12T07:41:43.344354Z",
     "iopub.status.idle": "2023-04-12T07:41:43.378533Z",
     "shell.execute_reply": "2023-04-12T07:41:43.377570Z",
     "shell.execute_reply.started": "2023-04-12T07:41:43.344618Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07dc2be-9fe5-4f2c-912b-632626faa5e5",
   "metadata": {},
   "source": [
    "# Create initial dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02836384-27d7-4a1a-89ae-e03dfcebd7d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:41:43.381279Z",
     "iopub.status.busy": "2023-04-12T07:41:43.380914Z",
     "iopub.status.idle": "2023-04-12T07:50:25.760649Z",
     "shell.execute_reply": "2023-04-12T07:50:25.759908Z",
     "shell.execute_reply.started": "2023-04-12T07:41:43.381250Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 07:41:44 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 07:42:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:42:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:42:36 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:42:51 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 07:43:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:43:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:43:36 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:43:51 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 07:44:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:44:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:44:36 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:44:51 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 07:45:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:45:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:45:36 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:45:51 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 07:46:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:46:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:46:36 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:46:51 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 07:47:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:47:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:47:36 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:47:51 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 07:48:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:48:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:48:36 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:48:51 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 07:49:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:49:21 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:49:36 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "23/04/12 07:49:51 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 07:50:06 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(\"s3a://incoming/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2763d99-7226-4551-a3ff-4d349746b44e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:50:25.762251Z",
     "iopub.status.busy": "2023-04-12T07:50:25.761681Z",
     "iopub.status.idle": "2023-04-12T07:50:25.787488Z",
     "shell.execute_reply": "2023-04-12T07:50:25.786680Z",
     "shell.execute_reply.started": "2023-04-12T07:50:25.762223Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"source_filename\", F.input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "979c9109-f16f-42d7-bdf9-d21c4db30903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:50:57.125534Z",
     "iopub.status.busy": "2023-04-12T07:50:57.124739Z",
     "iopub.status.idle": "2023-04-12T07:50:57.940736Z",
     "shell.execute_reply": "2023-04-12T07:50:57.939629Z",
     "shell.execute_reply.started": "2023-04-12T07:50:57.125502Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------------------\n",
      " vendor_id           | 1                                                 \n",
      " pickup_datetime     | 2019-01-15 03:36:12                               \n",
      " dropoff_datetime    | 2019-01-15 03:42:19                               \n",
      " passenger_count     | 1                                                 \n",
      " pickup_location_id  | 230                                               \n",
      " dropoff_location_id | 48                                                \n",
      " fare_amount         | 6.5                                               \n",
      " source_filename     | s3a://incoming/yellow_tripdata_sample_2019_01.csv \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "321d5b32-80f8-4ed3-8180-1b202fe534b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:51:37.065163Z",
     "iopub.status.busy": "2023-04-12T07:51:37.064800Z",
     "iopub.status.idle": "2023-04-12T07:51:39.979754Z",
     "shell.execute_reply": "2023-04-12T07:51:39.978817Z",
     "shell.execute_reply.started": "2023-04-12T07:51:37.065134Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .save(\"s3a://lake/taxis\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0806acb-cd57-40ef-9004-22b85b8c26b8",
   "metadata": {},
   "source": [
    "# Check schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28849d4d-80a2-4d4c-b46c-f9d282f4890f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:50:38.737194Z",
     "iopub.status.busy": "2023-04-12T07:50:38.736691Z",
     "iopub.status.idle": "2023-04-12T07:50:38.847913Z",
     "shell.execute_reply": "2023-04-12T07:50:38.846565Z",
     "shell.execute_reply.started": "2023-04-12T07:50:38.737153Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl = DeltaTable.forPath(spark, \"s3a://lake/taxis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91f5761e-f71c-45c9-82ef-df1d4b55417b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:50:38.850295Z",
     "iopub.status.busy": "2023-04-12T07:50:38.849576Z",
     "iopub.status.idle": "2023-04-12T07:50:40.361521Z",
     "shell.execute_reply": "2023-04-12T07:50:40.359925Z",
     "shell.execute_reply.started": "2023-04-12T07:50:38.850248Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------\n",
      " version             | 0                                                            \n",
      " timestamp           | 2023-04-12 07:50:29                                          \n",
      " userId              | null                                                         \n",
      " userName            | null                                                         \n",
      " operation           | WRITE                                                        \n",
      " operationParameters | {mode -> ErrorIfExists, partitionBy -> []}                   \n",
      " job                 | null                                                         \n",
      " notebook            | null                                                         \n",
      " clusterId           | null                                                         \n",
      " readVersion         | null                                                         \n",
      " isolationLevel      | Serializable                                                 \n",
      " isBlindAppend       | true                                                         \n",
      " operationMetrics    | {numFiles -> 1, numOutputRows -> 15, numOutputBytes -> 3449} \n",
      " userMetadata        | null                                                         \n",
      " engineInfo          | Apache-Spark/3.3.0 Delta-Lake/2.1.0                          \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dl.history().show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfdc55b-15e5-453c-b0c3-256026db919e",
   "metadata": {},
   "source": [
    "# Check data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3c90351-aecb-4a9e-84f4-541f69094e2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:50:40.364558Z",
     "iopub.status.busy": "2023-04-12T07:50:40.363512Z",
     "iopub.status.idle": "2023-04-12T07:50:40.419425Z",
     "shell.execute_reply": "2023-04-12T07:50:40.417821Z",
     "shell.execute_reply.started": "2023-04-12T07:50:40.364500Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://lake/taxis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bee189d-437d-432b-b92f-5290f9ceba28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:50:40.422670Z",
     "iopub.status.busy": "2023-04-12T07:50:40.421284Z",
     "iopub.status.idle": "2023-04-12T07:50:42.792879Z",
     "shell.execute_reply": "2023-04-12T07:50:42.791236Z",
     "shell.execute_reply.started": "2023-04-12T07:50:40.422611Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------------+---------------+------------------+-------------------+-----------+--------------------+\n",
      "|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|pickup_location_id|dropoff_location_id|fare_amount|     source_filename|\n",
      "+---------+-------------------+-------------------+---------------+------------------+-------------------+-----------+--------------------+\n",
      "|        1|2019-01-15 03:36:12|2019-01-15 03:42:19|              1|               230|                 48|        6.5|s3a://incoming/ye...|\n",
      "|        1|2019-01-25 18:20:32|2019-01-25 18:26:55|              1|               112|                112|        6.0|s3a://incoming/ye...|\n",
      "|        1|2019-01-05 06:47:31|2019-01-05 06:52:19|              1|               107|                  4|        6.0|s3a://incoming/ye...|\n",
      "|        1|2019-01-09 15:08:02|2019-01-09 15:20:17|              1|               143|                158|       11.0|s3a://incoming/ye...|\n",
      "|        1|2019-01-25 18:49:51|2019-01-25 18:56:44|              1|               246|                 90|        6.5|s3a://incoming/ye...|\n",
      "|        1|2019-01-26 18:24:17|2019-01-26 18:39:27|              1|               144|                 45|       10.5|s3a://incoming/ye...|\n",
      "|        1|2019-01-18 15:24:25|2019-01-18 15:40:01|              1|                13|                 68|       14.5|s3a://incoming/ye...|\n",
      "|        1|2019-01-03 14:56:38|2019-01-03 15:10:30|              1|               239|                237|       10.5|s3a://incoming/ye...|\n",
      "|        1|2019-01-04 15:50:52|2019-01-04 16:36:47|              1|               132|                238|       52.0|s3a://incoming/ye...|\n",
      "|        1|2019-01-05 17:55:44|2019-01-05 18:10:41|              1|               264|                264|       10.0|s3a://incoming/ye...|\n",
      "|        1|2019-01-20 22:28:16|2019-01-20 22:37:15|              1|                90|                229|        9.5|s3a://incoming/ye...|\n",
      "|        1|2019-01-17 09:50:15|2019-01-17 09:55:11|              1|               264|                264|        6.0|s3a://incoming/ye...|\n",
      "|        1|2019-01-22 14:43:21|2019-01-22 14:52:39|              1|               237|                236|        7.5|s3a://incoming/ye...|\n",
      "|        1|2019-01-29 20:39:15|2019-01-29 20:48:39|              1|               162|                246|        8.0|s3a://incoming/ye...|\n",
      "|        1|2019-01-03 21:09:26|2019-01-03 21:21:00|              1|               164|                125|       10.5|s3a://incoming/ye...|\n",
      "+---------+-------------------+-------------------+---------------+------------------+-------------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93cd22c-3d18-47fd-81e5-d9b0a9fb9b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
