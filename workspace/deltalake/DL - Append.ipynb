{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c998c14f-7129-47bb-9c04-8a1ec669433d",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bc5a55-5b35-4e38-a13b-b094a11ac7d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:55:00.841615Z",
     "iopub.status.busy": "2023-04-12T07:55:00.841069Z",
     "iopub.status.idle": "2023-04-12T07:55:00.890772Z",
     "shell.execute_reply": "2023-04-12T07:55:00.890101Z",
     "shell.execute_reply.started": "2023-04-12T07:55:00.841560Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b316699-7d96-4c97-9262-5bcf7fcdc811",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7dff68-aad3-4529-9575-e887774dd29d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:55:01.625155Z",
     "iopub.status.busy": "2023-04-12T07:55:01.624399Z",
     "iopub.status.idle": "2023-04-12T07:55:06.666837Z",
     "shell.execute_reply": "2023-04-12T07:55:06.665396Z",
     "shell.execute_reply.started": "2023-04-12T07:55:01.625121Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fade57d2-3cb4-46d1-9346-f760d734452f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 288ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fade57d2-3cb4-46d1-9346-f760d734452f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/7ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 07:55:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"pyspark-notebook\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a789b784-bab8-4c46-8763-7783151b444c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:55:32.371716Z",
     "iopub.status.busy": "2023-04-12T07:55:32.371175Z",
     "iopub.status.idle": "2023-04-12T07:55:32.383239Z",
     "shell.execute_reply": "2023-04-12T07:55:32.382175Z",
     "shell.execute_reply.started": "2023-04-12T07:55:32.371665Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"minio\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"minio123\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://172.18.0.2:9000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b123370-590c-4664-bc9f-977f5ff634d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:55:34.012770Z",
     "iopub.status.busy": "2023-04-12T07:55:34.012146Z",
     "iopub.status.idle": "2023-04-12T07:55:34.049068Z",
     "shell.execute_reply": "2023-04-12T07:55:34.048422Z",
     "shell.execute_reply.started": "2023-04-12T07:55:34.012719Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ade51-a17c-4e4b-a3a3-2c3f1dd6e7de",
   "metadata": {},
   "source": [
    "# Load DL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e732df1c-fdc7-489e-9d0d-257c4077850f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:55:35.762668Z",
     "iopub.status.busy": "2023-04-12T07:55:35.761640Z",
     "iopub.status.idle": "2023-04-12T07:55:54.675638Z",
     "shell.execute_reply": "2023-04-12T07:55:54.674215Z",
     "shell.execute_reply.started": "2023-04-12T07:55:35.762631Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 07:55:36 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dl = DeltaTable.forPath(spark, \"s3a://lake/taxis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07dc2be-9fe5-4f2c-912b-632626faa5e5",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02836384-27d7-4a1a-89ae-e03dfcebd7d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:59:41.196317Z",
     "iopub.status.busy": "2023-04-12T07:59:41.195894Z",
     "iopub.status.idle": "2023-04-12T07:59:41.411719Z",
     "shell.execute_reply": "2023-04-12T07:59:41.410752Z",
     "shell.execute_reply.started": "2023-04-12T07:59:41.196281Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"s3a://incoming/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7730c386-5b25-448c-bc16-681c1a2d5a4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:59:41.533949Z",
     "iopub.status.busy": "2023-04-12T07:59:41.533693Z",
     "iopub.status.idle": "2023-04-12T07:59:41.542335Z",
     "shell.execute_reply": "2023-04-12T07:59:41.541619Z",
     "shell.execute_reply.started": "2023-04-12T07:59:41.533933Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"source_filename\", F.input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5965970b-d709-4ae3-9128-d650af9eee66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:59:41.906209Z",
     "iopub.status.busy": "2023-04-12T07:59:41.905882Z",
     "iopub.status.idle": "2023-04-12T07:59:42.092598Z",
     "shell.execute_reply": "2023-04-12T07:59:42.091738Z",
     "shell.execute_reply.started": "2023-04-12T07:59:41.906185Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "206bafb6-b2d9-4526-8479-87154a25ce48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T07:59:43.024329Z",
     "iopub.status.busy": "2023-04-12T07:59:43.023771Z",
     "iopub.status.idle": "2023-04-12T07:59:43.196761Z",
     "shell.execute_reply": "2023-04-12T07:59:43.196065Z",
     "shell.execute_reply.started": "2023-04-12T07:59:43.024282Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------\n",
      " vendor_id           | 1                    \n",
      " pickup_datetime     | 2019-01-15 03:36:12  \n",
      " dropoff_datetime    | 2019-01-15 03:42:19  \n",
      " passenger_count     | 1                    \n",
      " pickup_location_id  | 230                  \n",
      " dropoff_location_id | 48                   \n",
      " fare_amount         | 6.5                  \n",
      " source_filename     | s3a://incoming/ye... \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b19cc-0758-4087-8913-3e9e35c6c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.format(\"delta\").mode(\"append\").save(\"s3a://lake/taxis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bce232-1b78-4117-9f45-992eab45f0c1",
   "metadata": {},
   "source": [
    "# Append to DL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "776b9bc3-4c0a-4e1f-b046-d74df899d178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T08:02:41.852566Z",
     "iopub.status.busy": "2023-04-12T08:02:41.851858Z",
     "iopub.status.idle": "2023-04-12T08:02:49.797050Z",
     "shell.execute_reply": "2023-04-12T08:02:49.795779Z",
     "shell.execute_reply.started": "2023-04-12T08:02:41.852533Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/12 08:02:45 WARN MergeIntoCommand: Merge source has SQLMetric(id: 490, name: Some(number of source rows), value: 20) rows in initial scan but SQLMetric(id: 491, name: Some(number of source rows (during repeated scan)), value: 0) rows in second scan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    dl\n",
    "    .alias(\"lake\")\n",
    "    .merge(df.alias(\"incoming\"),\"lake.source_filename = incoming.source_filename\")\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13ad9e96-a656-4d23-8adf-62ce96fbea7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T08:02:49.801022Z",
     "iopub.status.busy": "2023-04-12T08:02:49.800388Z",
     "iopub.status.idle": "2023-04-12T08:02:51.038273Z",
     "shell.execute_reply": "2023-04-12T08:02:51.037541Z",
     "shell.execute_reply.started": "2023-04-12T08:02:49.800970Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 1                                                                                                                                                                                                                                                                                                              \n",
      " timestamp           | 2023-04-12 08:02:45                                                                                                                                                                                                                                                                                            \n",
      " userId              | null                                                                                                                                                                                                                                                                                                           \n",
      " userName            | null                                                                                                                                                                                                                                                                                                           \n",
      " operation           | MERGE                                                                                                                                                                                                                                                                                                          \n",
      " operationParameters | {predicate -> (lake.source_filename = incoming.source_filename), matchedPredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}                                                                                                                                                                   \n",
      " job                 | null                                                                                                                                                                                                                                                                                                           \n",
      " notebook            | null                                                                                                                                                                                                                                                                                                           \n",
      " clusterId           | null                                                                                                                                                                                                                                                                                                           \n",
      " readVersion         | 0                                                                                                                                                                                                                                                                                                              \n",
      " isolationLevel      | Serializable                                                                                                                                                                                                                                                                                                   \n",
      " isBlindAppend       | false                                                                                                                                                                                                                                                                                                          \n",
      " operationMetrics    | {numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, executionTimeMs -> 3328, numTargetRowsInserted -> 20, scanTimeMs -> 0, numTargetRowsUpdated -> 0, numOutputRows -> 20, numTargetChangeFilesAdded -> 0, numSourceRows -> 20, numTargetFilesRemoved -> 0, rewriteTimeMs -> 3293} \n",
      " userMetadata        | null                                                                                                                                                                                                                                                                                                           \n",
      " engineInfo          | Apache-Spark/3.3.0 Delta-Lake/2.1.0                                                                                                                                                                                                                                                                            \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 0                                                                                                                                                                                                                                                                                                              \n",
      " timestamp           | 2023-04-12 07:51:37                                                                                                                                                                                                                                                                                            \n",
      " userId              | null                                                                                                                                                                                                                                                                                                           \n",
      " userName            | null                                                                                                                                                                                                                                                                                                           \n",
      " operation           | WRITE                                                                                                                                                                                                                                                                                                          \n",
      " operationParameters | {mode -> ErrorIfExists, partitionBy -> []}                                                                                                                                                                                                                                                                     \n",
      " job                 | null                                                                                                                                                                                                                                                                                                           \n",
      " notebook            | null                                                                                                                                                                                                                                                                                                           \n",
      " clusterId           | null                                                                                                                                                                                                                                                                                                           \n",
      " readVersion         | null                                                                                                                                                                                                                                                                                                           \n",
      " isolationLevel      | Serializable                                                                                                                                                                                                                                                                                                   \n",
      " isBlindAppend       | false                                                                                                                                                                                                                                                                                                          \n",
      " operationMetrics    | {numFiles -> 1, numOutputRows -> 0, numOutputBytes -> 1055}                                                                                                                                                                                                                                                    \n",
      " userMetadata        | null                                                                                                                                                                                                                                                                                                           \n",
      " engineInfo          | Apache-Spark/3.3.0 Delta-Lake/2.1.0                                                                                                                                                                                                                                                                            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dl.history().show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7251a5-0b17-4230-8f11-f1fa3ca684d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read again DL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb27ce1c-81ee-4c4b-bf68-db066a14c3b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T08:02:56.334551Z",
     "iopub.status.busy": "2023-04-12T08:02:56.334001Z",
     "iopub.status.idle": "2023-04-12T08:02:56.367415Z",
     "shell.execute_reply": "2023-04-12T08:02:56.366617Z",
     "shell.execute_reply.started": "2023-04-12T08:02:56.334519Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://lake/taxis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d37b46c7-3cea-4fa2-baae-8422a0b37b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-12T08:02:56.577009Z",
     "iopub.status.busy": "2023-04-12T08:02:56.576275Z",
     "iopub.status.idle": "2023-04-12T08:02:57.482245Z",
     "shell.execute_reply": "2023-04-12T08:02:57.481385Z",
     "shell.execute_reply.started": "2023-04-12T08:02:56.576978Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db2c82b-56cc-4434-867c-1881c58bb603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
