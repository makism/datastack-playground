{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c998c14f-7129-47bb-9c04-8a1ec669433d",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bc5a55-5b35-4e38-a13b-b094a11ac7d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T05:55:39.420050Z",
     "iopub.status.busy": "2023-04-13T05:55:39.419656Z",
     "iopub.status.idle": "2023-04-13T05:55:39.486455Z",
     "shell.execute_reply": "2023-04-13T05:55:39.485716Z",
     "shell.execute_reply.started": "2023-04-13T05:55:39.420017Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b316699-7d96-4c97-9262-5bcf7fcdc811",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7dff68-aad3-4529-9575-e887774dd29d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T05:55:40.273940Z",
     "iopub.status.busy": "2023-04-13T05:55:40.273700Z",
     "iopub.status.idle": "2023-04-13T05:55:45.631835Z",
     "shell.execute_reply": "2023-04-13T05:55:45.630975Z",
     "shell.execute_reply.started": "2023-04-13T05:55:40.273921Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9a78bf29-87a7-4361-8a34-91370917daa9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 278ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9a78bf29-87a7-4361-8a34-91370917daa9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/6ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/13 05:55:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"pyspark-notebook\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a789b784-bab8-4c46-8763-7783151b444c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T05:55:45.634072Z",
     "iopub.status.busy": "2023-04-13T05:55:45.633680Z",
     "iopub.status.idle": "2023-04-13T05:55:45.642023Z",
     "shell.execute_reply": "2023-04-13T05:55:45.641235Z",
     "shell.execute_reply.started": "2023-04-13T05:55:45.634037Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"minio\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"minio123\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://172.18.0.2:9000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b123370-590c-4664-bc9f-977f5ff634d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T05:55:45.643408Z",
     "iopub.status.busy": "2023-04-13T05:55:45.643091Z",
     "iopub.status.idle": "2023-04-13T05:55:45.677375Z",
     "shell.execute_reply": "2023-04-13T05:55:45.676484Z",
     "shell.execute_reply.started": "2023-04-13T05:55:45.643379Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ade51-a17c-4e4b-a3a3-2c3f1dd6e7de",
   "metadata": {},
   "source": [
    "# Load DL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e732df1c-fdc7-489e-9d0d-257c4077850f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T06:10:03.442493Z",
     "iopub.status.busy": "2023-04-13T06:10:03.441630Z",
     "iopub.status.idle": "2023-04-13T06:10:39.808258Z",
     "shell.execute_reply": "2023-04-13T06:10:39.807029Z",
     "shell.execute_reply.started": "2023-04-13T06:10:03.442435Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/13 06:10:05 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dl = DeltaTable.forPath(spark, \"s3a://lake/taxis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07dc2be-9fe5-4f2c-912b-632626faa5e5",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02836384-27d7-4a1a-89ae-e03dfcebd7d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T06:14:04.328934Z",
     "iopub.status.busy": "2023-04-13T06:14:04.328449Z",
     "iopub.status.idle": "2023-04-13T06:14:04.600541Z",
     "shell.execute_reply": "2023-04-13T06:14:04.599154Z",
     "shell.execute_reply.started": "2023-04-13T06:14:04.328894Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"s3a://incoming/yellow_tripdata_sample_2019_03_new_schema.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7730c386-5b25-448c-bc16-681c1a2d5a4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T06:14:04.882353Z",
     "iopub.status.busy": "2023-04-13T06:14:04.881885Z",
     "iopub.status.idle": "2023-04-13T06:14:04.893523Z",
     "shell.execute_reply": "2023-04-13T06:14:04.892442Z",
     "shell.execute_reply.started": "2023-04-13T06:14:04.882311Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"source_filename\", F.input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5965970b-d709-4ae3-9128-d650af9eee66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T06:14:08.753866Z",
     "iopub.status.busy": "2023-04-13T06:14:08.753358Z",
     "iopub.status.idle": "2023-04-13T06:14:08.882676Z",
     "shell.execute_reply": "2023-04-13T06:14:08.881946Z",
     "shell.execute_reply.started": "2023-04-13T06:14:08.753842Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "206bafb6-b2d9-4526-8479-87154a25ce48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T06:14:09.697635Z",
     "iopub.status.busy": "2023-04-13T06:14:09.697279Z",
     "iopub.status.idle": "2023-04-13T06:14:09.844342Z",
     "shell.execute_reply": "2023-04-13T06:14:09.843201Z",
     "shell.execute_reply.started": "2023-04-13T06:14:09.697605Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------\n",
      " vendor_id           | 2                    \n",
      " pickup_datetime     | 2019-02-28 00:39:50  \n",
      " dropoff_datetime    | 2019-02-28 00:44:23  \n",
      " passenger_count     | 5                    \n",
      " pickup_location_id  | 79                   \n",
      " dropoff_location_id | 148                  \n",
      " fare_amount         | 5.0                  \n",
      " stars               | 5                    \n",
      " source_filename     | s3a://incoming/ye... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a5fefb9-eae3-4b93-8d47-a20364382a73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T06:14:11.966693Z",
     "iopub.status.busy": "2023-04-13T06:14:11.966355Z",
     "iopub.status.idle": "2023-04-13T06:14:11.971121Z",
     "shell.execute_reply": "2023-04-13T06:14:11.970357Z",
     "shell.execute_reply.started": "2023-04-13T06:14:11.966664Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- pickup_location_id: string (nullable = true)\n",
      " |-- dropoff_location_id: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      " |-- source_filename: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bce232-1b78-4117-9f45-992eab45f0c1",
   "metadata": {},
   "source": [
    "# Append to DL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b84194c-0629-45a0-8bbb-b61213cc6523",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T06:15:21.776761Z",
     "iopub.status.busy": "2023-04-13T06:15:21.776503Z",
     "iopub.status.idle": "2023-04-13T06:15:21.836867Z",
     "shell.execute_reply": "2023-04-13T06:15:21.836007Z",
     "shell.execute_reply.started": "2023-04-13T06:15:21.776740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable automatic schema evolution\n",
    "spark.sql(\"SET spark.databricks.delta.schema.autoMerge.enabled = true\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebd0f8cf-20e7-4f73-9ee7-4ff5eeae849f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T06:15:38.982158Z",
     "iopub.status.busy": "2023-04-13T06:15:38.981552Z",
     "iopub.status.idle": "2023-04-13T06:15:38.985828Z",
     "shell.execute_reply": "2023-04-13T06:15:38.984916Z",
     "shell.execute_reply.started": "2023-04-13T06:15:38.982129Z"
    }
   },
   "outputs": [],
   "source": [
    "# new_data.write.option(\"mergeSchema\", \"true\").mode(\"append\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "776b9bc3-4c0a-4e1f-b046-d74df899d178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T06:15:45.969250Z",
     "iopub.status.busy": "2023-04-13T06:15:45.968983Z",
     "iopub.status.idle": "2023-04-13T06:15:56.870497Z",
     "shell.execute_reply": "2023-04-13T06:15:56.869485Z",
     "shell.execute_reply.started": "2023-04-13T06:15:45.969229Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/13 06:15:51 WARN MergeIntoCommand: Merge source has SQLMetric(id: 841, name: Some(number of source rows), value: 1) rows in initial scan but SQLMetric(id: 842, name: Some(number of source rows (during repeated scan)), value: 0) rows in second scan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    dl\n",
    "    .alias(\"lake\")\n",
    "    .merge(df.alias(\"incoming\"),\"lake.source_filename = incoming.source_filename\")\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13ad9e96-a656-4d23-8adf-62ce96fbea7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T06:15:56.873375Z",
     "iopub.status.busy": "2023-04-13T06:15:56.872522Z",
     "iopub.status.idle": "2023-04-13T06:15:58.402114Z",
     "shell.execute_reply": "2023-04-13T06:15:58.400594Z",
     "shell.execute_reply.started": "2023-04-13T06:15:56.873331Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 2                                                                                                                                                                                                                                                                                                              \n",
      " timestamp           | 2023-04-13 06:15:52                                                                                                                                                                                                                                                                                            \n",
      " userId              | null                                                                                                                                                                                                                                                                                                           \n",
      " userName            | null                                                                                                                                                                                                                                                                                                           \n",
      " operation           | MERGE                                                                                                                                                                                                                                                                                                          \n",
      " operationParameters | {predicate -> (lake.source_filename = incoming.source_filename), matchedPredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}                                                                                                                                                                   \n",
      " job                 | null                                                                                                                                                                                                                                                                                                           \n",
      " notebook            | null                                                                                                                                                                                                                                                                                                           \n",
      " clusterId           | null                                                                                                                                                                                                                                                                                                           \n",
      " readVersion         | 1                                                                                                                                                                                                                                                                                                              \n",
      " isolationLevel      | Serializable                                                                                                                                                                                                                                                                                                   \n",
      " isBlindAppend       | false                                                                                                                                                                                                                                                                                                          \n",
      " operationMetrics    | {numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, executionTimeMs -> 5611, numTargetRowsInserted -> 1, scanTimeMs -> 0, numTargetRowsUpdated -> 0, numOutputRows -> 1, numTargetChangeFilesAdded -> 0, numSourceRows -> 1, numTargetFilesRemoved -> 0, rewriteTimeMs -> 5517}    \n",
      " userMetadata        | null                                                                                                                                                                                                                                                                                                           \n",
      " engineInfo          | Apache-Spark/3.3.0 Delta-Lake/2.1.0                                                                                                                                                                                                                                                                            \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 1                                                                                                                                                                                                                                                                                                              \n",
      " timestamp           | 2023-04-12 08:02:45                                                                                                                                                                                                                                                                                            \n",
      " userId              | null                                                                                                                                                                                                                                                                                                           \n",
      " userName            | null                                                                                                                                                                                                                                                                                                           \n",
      " operation           | MERGE                                                                                                                                                                                                                                                                                                          \n",
      " operationParameters | {predicate -> (lake.source_filename = incoming.source_filename), matchedPredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}                                                                                                                                                                   \n",
      " job                 | null                                                                                                                                                                                                                                                                                                           \n",
      " notebook            | null                                                                                                                                                                                                                                                                                                           \n",
      " clusterId           | null                                                                                                                                                                                                                                                                                                           \n",
      " readVersion         | 0                                                                                                                                                                                                                                                                                                              \n",
      " isolationLevel      | Serializable                                                                                                                                                                                                                                                                                                   \n",
      " isBlindAppend       | false                                                                                                                                                                                                                                                                                                          \n",
      " operationMetrics    | {numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, executionTimeMs -> 3328, numTargetRowsInserted -> 20, scanTimeMs -> 0, numTargetRowsUpdated -> 0, numOutputRows -> 20, numTargetChangeFilesAdded -> 0, numSourceRows -> 20, numTargetFilesRemoved -> 0, rewriteTimeMs -> 3293} \n",
      " userMetadata        | null                                                                                                                                                                                                                                                                                                           \n",
      " engineInfo          | Apache-Spark/3.3.0 Delta-Lake/2.1.0                                                                                                                                                                                                                                                                            \n",
      "-RECORD 2-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " version             | 0                                                                                                                                                                                                                                                                                                              \n",
      " timestamp           | 2023-04-12 07:51:37                                                                                                                                                                                                                                                                                            \n",
      " userId              | null                                                                                                                                                                                                                                                                                                           \n",
      " userName            | null                                                                                                                                                                                                                                                                                                           \n",
      " operation           | WRITE                                                                                                                                                                                                                                                                                                          \n",
      " operationParameters | {mode -> ErrorIfExists, partitionBy -> []}                                                                                                                                                                                                                                                                     \n",
      " job                 | null                                                                                                                                                                                                                                                                                                           \n",
      " notebook            | null                                                                                                                                                                                                                                                                                                           \n",
      " clusterId           | null                                                                                                                                                                                                                                                                                                           \n",
      " readVersion         | null                                                                                                                                                                                                                                                                                                           \n",
      " isolationLevel      | Serializable                                                                                                                                                                                                                                                                                                   \n",
      " isBlindAppend       | false                                                                                                                                                                                                                                                                                                          \n",
      " operationMetrics    | {numFiles -> 1, numOutputRows -> 0, numOutputBytes -> 1055}                                                                                                                                                                                                                                                    \n",
      " userMetadata        | null                                                                                                                                                                                                                                                                                                           \n",
      " engineInfo          | Apache-Spark/3.3.0 Delta-Lake/2.1.0                                                                                                                                                                                                                                                                            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dl.history().show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7251a5-0b17-4230-8f11-f1fa3ca684d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read again DL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb27ce1c-81ee-4c4b-bf68-db066a14c3b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T06:16:11.855222Z",
     "iopub.status.busy": "2023-04-13T06:16:11.854989Z",
     "iopub.status.idle": "2023-04-13T06:16:11.884753Z",
     "shell.execute_reply": "2023-04-13T06:16:11.883985Z",
     "shell.execute_reply.started": "2023-04-13T06:16:11.855202Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"s3a://lake/taxis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d37b46c7-3cea-4fa2-baae-8422a0b37b30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T06:16:12.263634Z",
     "iopub.status.busy": "2023-04-13T06:16:12.263277Z",
     "iopub.status.idle": "2023-04-13T06:16:13.366819Z",
     "shell.execute_reply": "2023-04-13T06:16:13.365689Z",
     "shell.execute_reply.started": "2023-04-13T06:16:12.263603Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4db2c82b-56cc-4434-867c-1881c58bb603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-13T06:16:22.429490Z",
     "iopub.status.busy": "2023-04-13T06:16:22.428894Z",
     "iopub.status.idle": "2023-04-13T06:16:22.433711Z",
     "shell.execute_reply": "2023-04-13T06:16:22.432934Z",
     "shell.execute_reply.started": "2023-04-13T06:16:22.429466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- pickup_location_id: string (nullable = true)\n",
      " |-- dropoff_location_id: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- source_filename: string (nullable = true)\n",
      " |-- stars: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251471e2-2a31-48d3-bf55-703f2ecb4779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
